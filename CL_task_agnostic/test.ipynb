{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： d:\\Githubdesktop\\github\\thesis_code\\thesis_code\\CL_task_agnostic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"当前工作目录：\", os.getcwd())\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# from CL_task_agnostic.loss_surface import     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. 指定事件文件路径\n",
    "event_file = r\"D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\none\\criterion=GDL_lr=0.001_lr_reduce_batch=2_seed=43\\events.out.tfevents.1747542899.pc3054.3998252.0\"\n",
    "print(\"路径：\", event_file)\n",
    "print(\"存在？\", os.path.exists(event_file))\n",
    "print(\"是文件？\", os.path.isfile(event_file))\n",
    "if os.path.isfile(event_file):\n",
    "    print(\"大小 (bytes)：\", os.path.getsize(event_file))\n",
    "\n",
    "\n",
    "# 2. 加载：告诉它 scalars 全都读出来（0 表示不限制数量）\n",
    "ea = event_accumulator.EventAccumulator(\n",
    "    event_file,\n",
    "    size_guidance={event_accumulator.SCALARS: 0}\n",
    ")\n",
    "ea.Reload()\n",
    "\n",
    "# 3. 查看有哪些 scalar tags\n",
    "print(\"可用 tags:\", ea.Tags()[\"scalars\"])\n",
    "\n",
    "# 4. 取出 train_loss 对应的所有条目\n",
    "#    每个 entry 是一个 namedtuple： (wall_time, step, value)\n",
    "entries = ea.Scalars(\"total train loss\")\n",
    "\n",
    "# 5. 转成列表 / NumPy /DataFrame\n",
    "steps      = [e.step for e in entries]\n",
    "wall_times = [e.wall_time for e in entries]\n",
    "values     = [e.value for e in entries]\n",
    "\n",
    "# 作为 Python 列表\n",
    "loss_list_training = values\n",
    "print('loss_list len', len(loss_list_training))\n",
    "\n",
    "# 或转 NumPy 数组\n",
    "loss_arr = np.array(values)\n",
    "\n",
    "# Loss_surface detection\n",
    "count_updates=0\n",
    "loss_window_length = 8\n",
    "loss_window_mean_threshold = 0.25\n",
    "loss_window_variance_threshold = 1e-4\n",
    "jump_ratio_threshold = 0.5\n",
    "loss_window=[]\n",
    "loss_window_means=[]\n",
    "loss_window_variances=[]\n",
    "new_peak_detected = True\n",
    "num_batches = len(loss_list_training)\n",
    "curr_batch_loss = None\n",
    "prev_batch_loss = None\n",
    "for i in range(num_batches):\n",
    "    loss = loss_list_training[i]\n",
    "    loss_window.append(np.mean(loss))\n",
    "    if len(loss_window)>loss_window_length: del loss_window[0]\n",
    "    loss_window_mean=np.mean(loss_window)\n",
    "    loss_window_variance=np.var(loss_window)\n",
    "    # Save previous loss window mean and variance\n",
    "\n",
    "    ##\n",
    "    prev_batch_loss = curr_batch_loss\n",
    "    curr_batch_loss = loss\n",
    "    if prev_batch_loss is not None and curr_batch_loss is not None:\n",
    "        jump_ratio = (curr_batch_loss - prev_batch_loss) / prev_batch_loss \n",
    "\n",
    "    # print(f'batch: {i+1}loss window mean: {loss_window_mean}, loss window variance: {loss_window_variance} jump ratio: {jump_ratio}')\n",
    "    # if not new_peak_detected and loss_window_mean > last_loss_window_mean+np.sqrt(last_loss_window_variance) :\n",
    "    if not new_peak_detected and loss_window_mean > prev_loss_window_mean+np.sqrt(prev_loss_window_variance) and jump_ratio > jump_ratio_threshold:\n",
    "        new_peak_detected=True \n",
    "        count_updates+=1\n",
    "        print(f'new peak detected at {i+1} step')\n",
    "    \n",
    "    if loss_window_mean < loss_window_mean_threshold and loss_window_variance < loss_window_variance_threshold and new_peak_detected:\n",
    "        print(f'Detect loss plateau at {i+1} step for {count_updates} times: last loss window mean: {loss_window_mean}, last loss window variance: {loss_window_variance}')\n",
    "        last_loss_window_mean=loss_window_mean\n",
    "        last_loss_window_variance=loss_window_variance\n",
    "        new_peak_detected=False\n",
    "    \n",
    "    prev_loss_window_mean     = loss_window_mean\n",
    "    prev_loss_window_variance = loss_window_variance\n",
    "    \n",
    "print(f'count_updates: {count_updates}')\n",
    "\n",
    "\n",
    "\n",
    "# last loss window mean: 0.18679414689540863, last loss window variance: 8.96894832769668e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_loss_surface(loss_list_training, loss_window_length=8, loss_window_mean_threshold=0.25, loss_window_variance_threshold=1e-4, jump_ratio_threshold=0.5):\n",
    "    loss_window = []\n",
    "    jump_ratio_threshold = 0.5\n",
    "    count_updates=0\n",
    "    new_peak_detected = True\n",
    "    prev_batch_loss = None\n",
    "    curr_batch_loss = None\n",
    "    for i in range(len(loss_list_training)):\n",
    "        loss = loss_list_training[i]\n",
    "        loss_window.append(np.mean(loss))\n",
    "        if len(loss_window)>loss_window_length: del loss_window[0]\n",
    "        loss_window_mean=np.mean(loss_window)\n",
    "        loss_window_variance=np.var(loss_window)\n",
    "        # Save previous loss window mean and variance\n",
    "\n",
    "        ##\n",
    "        prev_batch_loss = curr_batch_loss\n",
    "        curr_batch_loss = loss\n",
    "        if prev_batch_loss is not None and curr_batch_loss is not None:\n",
    "            jump_ratio = (curr_batch_loss - prev_batch_loss) / prev_batch_loss \n",
    "\n",
    "        # print(f'batch: {i+1}loss window mean: {loss_window_mean}, loss window variance: {loss_window_variance} jump ratio: {jump_ratio}')\n",
    "        # if not new_peak_detected and loss_window_mean > last_loss_window_mean+np.sqrt(last_loss_window_variance) :\n",
    "        if not new_peak_detected :\n",
    "            if loss_window_mean > prev_loss_window_mean+np.sqrt(prev_loss_window_variance) and jump_ratio > jump_ratio_threshold:\n",
    "            # if loss_window_mean > prev_loss_window_mean+np.sqrt(prev_loss_window_variance) :\n",
    "                new_peak_detected=True \n",
    "                count_updates+=1\n",
    "                print(f'new peak detected at {i+1} step')\n",
    "        \n",
    "        if loss_window_mean < loss_window_mean_threshold and loss_window_variance < loss_window_variance_threshold and new_peak_detected:\n",
    "            print(f'Detect loss plateau at {i+1} step for {count_updates} times: last loss window mean: {loss_window_mean}, last loss window variance: {loss_window_variance}')\n",
    "            last_loss_window_mean=loss_window_mean\n",
    "            last_loss_window_variance=loss_window_variance\n",
    "            new_peak_detected=False\n",
    "        \n",
    "        prev_loss_window_mean     = loss_window_mean\n",
    "        prev_loss_window_variance = loss_window_variance\n",
    "        \n",
    "    print(f'count_updates: {count_updates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 4 个 GDL 目录：\n",
      "共找到 4 个 event 文件：\n",
      "   D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=42_ewc_lambda=0.4_ewc_alpha=0.9\\events.out.tfevents.1747572525.pc3054.3204022.0\n",
      "   D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=42_ewc_lambda=1.0_ewc_alpha=0.9\\events.out.tfevents.1747572490.pc3054.3196219.0\n",
      "   D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=43_ewc_lambda=1.0_ewc_alpha=0.9\\events.out.tfevents.1747572490.pc3054.3196278.0\n",
      "   D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=44_ewc_lambda=1.0_ewc_alpha=0.9\\events.out.tfevents.1747572513.pc3054.3196426.0\n",
      "D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=42_ewc_lambda=0.4_ewc_alpha=0.9\n",
      "loss_list len 900\n",
      "Detect loss plateau at 126 step for 0 times: last loss window mean: 0.19880135570253646, last loss window variance: 8.373384312647534e-05\n",
      "new peak detected at 301 step\n",
      "Detect loss plateau at 308 step for 1 times: last loss window mean: 0.1591173495565142, last loss window variance: 6.698469077084425e-05\n",
      "new peak detected at 601 step\n",
      "Detect loss plateau at 619 step for 2 times: last loss window mean: 0.16302136863980973, last loss window variance: 3.4129996430784344e-05\n",
      "count_updates: 2\n",
      "D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=42_ewc_lambda=1.0_ewc_alpha=0.9\n",
      "loss_list len 900\n",
      "Detect loss plateau at 126 step for 0 times: last loss window mean: 0.19880135570253646, last loss window variance: 8.373384312647534e-05\n",
      "new peak detected at 301 step\n",
      "Detect loss plateau at 308 step for 1 times: last loss window mean: 0.1583958693913051, last loss window variance: 7.28820516163855e-05\n",
      "new peak detected at 601 step\n",
      "Detect loss plateau at 619 step for 2 times: last loss window mean: 0.16155035155160086, last loss window variance: 4.0445418659605915e-05\n",
      "count_updates: 2\n",
      "D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=43_ewc_lambda=1.0_ewc_alpha=0.9\n",
      "loss_list len 900\n",
      "Detect loss plateau at 118 step for 0 times: last loss window mean: 0.21277768271309988, last loss window variance: 5.187818926234002e-05\n",
      "new peak detected at 301 step\n",
      "Detect loss plateau at 308 step for 1 times: last loss window mean: 0.16920930998665945, last loss window variance: 7.934659680426754e-05\n",
      "new peak detected at 601 step\n",
      "Detect loss plateau at 616 step for 2 times: last loss window mean: 0.17001188652856009, last loss window variance: 9.370453235434599e-05\n",
      "count_updates: 2\n",
      "D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\\criterion=GDL_lr=0.001_lr_reduce_batch=3_seed=44_ewc_lambda=1.0_ewc_alpha=0.9\n",
      "loss_list len 900\n",
      "Detect loss plateau at 102 step for 0 times: last loss window mean: 0.2496753420148577, last loss window variance: 8.226284819600397e-05\n",
      "new peak detected at 301 step\n",
      "Detect loss plateau at 311 step for 1 times: last loss window mean: 0.14541608095169067, last loss window variance: 7.834525691967948e-05\n",
      "new peak detected at 601 step\n",
      "Detect loss plateau at 618 step for 2 times: last loss window mean: 0.16098570823669434, last loss window variance: 6.991009648312943e-05\n",
      "count_updates: 2\n"
     ]
    }
   ],
   "source": [
    "# VSCode.Cell id=\"new_detect_gdl\" language=\"python\"\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "root_dir = r\"D:\\Githubdesktop\\github\\thesis_code\\thesis_code\\log_CL_task_agnostic\\ewc\"\n",
    "\n",
    "# 1) 递归查找所有以 criterion=GDL 开头的子目录\n",
    "gdl_dirs = []\n",
    "for root, dirs, _ in os.walk(root_dir):\n",
    "    for d in dirs:\n",
    "        if d.startswith(\"criterion=GDL\"):\n",
    "            gdl_dirs.append(os.path.join(root, d))\n",
    "print(f\"找到 {len(gdl_dirs)} 个 GDL 目录：\")\n",
    "# for d in gdl_dirs:\n",
    "    # print(\"  \", d)\n",
    "\n",
    "# 2) 在这些目录下收集所有 events.out.tfevents 文件\n",
    "event_files = []\n",
    "for d in gdl_dirs:\n",
    "    for fn in os.listdir(d):\n",
    "        if fn.startswith(\"events.out.tfevents\"):\n",
    "            event_files.append(os.path.join(d, fn))\n",
    "print(f\"共找到 {len(event_files)} 个 event 文件：\")\n",
    "for ef in event_files:\n",
    "    print(\"  \", ef)\n",
    "\n",
    "# 3) 验证加载并打印可用的 scalar tags\n",
    "i = 0\n",
    "loss_window_length = 7\n",
    "loss_window_mean_threshold = 0.25\n",
    "loss_window_variance_threshold = 1e-4\n",
    "jump_ratio_threshold = 0.5\n",
    "for ef in event_files:\n",
    "    try:\n",
    "        print(gdl_dirs[i])\n",
    "        i += 1  \n",
    "        ea = event_accumulator.EventAccumulator(\n",
    "            ef,\n",
    "            size_guidance={event_accumulator.SCALARS: 0}\n",
    "        )\n",
    "        ea.Reload()\n",
    "        entries = ea.Scalars(\"train loss\")\n",
    "\n",
    "        # 5. 转成列表 / NumPy /DataFrame\n",
    "        steps      = [e.step for e in entries]\n",
    "        wall_times = [e.wall_time for e in entries]\n",
    "        values     = [e.value for e in entries]\n",
    "\n",
    "        # 作为 Python 列表\n",
    "        loss_list_training = values\n",
    "        print('loss_list len', len(loss_list_training))\n",
    "\n",
    "        # 或转 NumPy 数组\n",
    "        loss_arr = np.array(values)\n",
    "        detect_loss_surface(loss_list_training, loss_window_length=loss_window_length, loss_window_mean_threshold=loss_window_mean_threshold, loss_window_variance_threshold=loss_window_variance_threshold, jump_ratio_threshold=jump_ratio_threshold)\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] {os.path.basename(ef)} → {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
