{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import peft\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from MLP_function import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "y.sum tensor(284)\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(45)\n",
    "# 3D CNN\n",
    "X = torch.rand((1000, 1, 28, 28, 28))\n",
    "y = (X.sum(dim=[1,2,3,4]) > 11000).long()  # Binary classification based on voxel sum\n",
    "\n",
    "print(y[0 :20])\n",
    "print('y.sum',y.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 800\n",
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X[:n_train], y[:n_train]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X[n_train:], y[n_train:]),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "lr = 0.002\n",
    "max_epochs = 30\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lora\n",
    "config = peft.LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"conv.0\", \"conv.3\", 'fc.0'],\n",
    "    # modules_to_save=[\"fc\"] ,\n",
    ")\n",
    "\n",
    "original_model = CNN3D().to(device)\n",
    "original_model_copy = copy.deepcopy(original_model)  # we keep a copy of the original model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal_model type <class 'MLP_function.CNN3D'>\n",
      "before build peft model==========\n",
      "trainalbe parameter in orginal_model:\n",
      "Layer: conv.0.weight | Trainable: True\n",
      "Layer: conv.0.bias | Trainable: True\n",
      "Layer: conv.3.weight | Trainable: True\n",
      "Layer: conv.3.bias | Trainable: True\n",
      "Layer: fc.0.weight | Trainable: True\n",
      "Layer: fc.0.bias | Trainable: True\n",
      "Layer: fc.2.weight | Trainable: True\n",
      "Layer: fc.2.bias | Trainable: True\n",
      "after build peft model==========\n",
      "trainalbe parameter in peft_model:\n",
      "Layer: base_model.model.conv.0.base_layer.weight | Trainable: False\n",
      "Layer: base_model.model.conv.0.base_layer.bias | Trainable: False\n",
      "Layer: base_model.model.conv.0.lora_A.default.weight | Trainable: True\n",
      "Layer: base_model.model.conv.0.lora_B.default.weight | Trainable: True\n",
      "Layer: base_model.model.conv.3.base_layer.weight | Trainable: False\n",
      "Layer: base_model.model.conv.3.base_layer.bias | Trainable: False\n",
      "Layer: base_model.model.conv.3.lora_A.default.weight | Trainable: True\n",
      "Layer: base_model.model.conv.3.lora_B.default.weight | Trainable: True\n",
      "Layer: base_model.model.fc.0.base_layer.weight | Trainable: False\n",
      "Layer: base_model.model.fc.0.base_layer.bias | Trainable: False\n",
      "Layer: base_model.model.fc.0.lora_A.default.weight | Trainable: True\n",
      "Layer: base_model.model.fc.0.lora_B.default.weight | Trainable: True\n",
      "Layer: base_model.model.fc.2.weight | Trainable: False\n",
      "Layer: base_model.model.fc.2.bias | Trainable: False\n",
      "trainable params: 92,664 || all params: 1,204,870 || trainable%: 7.6908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3D CNN\n",
    "## \n",
    "print('orginal_model type',type(original_model))\n",
    "# print('orginal_model_copy type',type(original_model_copy))\n",
    "\n",
    "print('before build peft model==========')\n",
    "print('trainalbe parameter in orginal_model:')\n",
    "for name, param in original_model.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")\n",
    "\n",
    "# print(\"可训练参数列表:\")\n",
    "# for name, param in original_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name}: {param.shape}\")\n",
    "\n",
    "print('after build peft model==========')\n",
    "peft_model = peft.get_peft_model(original_model_copy, config)\n",
    "optimizer = optim.Adam(peft_model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "print('trainalbe parameter in peft_model:')\n",
    "for name, param in peft_model.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")\n",
    "\n",
    "# print(\"可训练参数列表:\")\n",
    "# for name, param in peft_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name}: {param.shape}\")\n",
    "\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 get train loss: 0.6353276371955872 val loss: 0.5333637595176697\n",
      "Batch 1 get train loss: 0.6056520342826843 val loss: 0.5178259611129761\n",
      "Batch 2 get train loss: 0.6068936586380005 val loss: 0.5324974656105042\n",
      "Batch 3 get train loss: 0.6043715476989746 val loss: 0.5260958671569824\n",
      "Batch 4 get train loss: 0.6008870601654053 val loss: 0.5142931342124939\n",
      "Batch 5 get train loss: 0.6099104285240173 val loss: 0.5510181188583374\n",
      "Batch 6 get train loss: 0.5951511263847351 val loss: 0.5079817175865173\n",
      "Batch 7 get train loss: 0.6089515686035156 val loss: 0.5311503410339355\n",
      "Batch 8 get train loss: 0.6029284596443176 val loss: 0.5400116443634033\n",
      "Batch 9 get train loss: 0.6032438278198242 val loss: 0.5250625014305115\n",
      "Batch 10 get train loss: 0.6046921610832214 val loss: 0.5064818263053894\n",
      "Batch 11 get train loss: 0.5955328345298767 val loss: 0.5295922756195068\n",
      "Batch 12 get train loss: 0.5906210541725159 val loss: 0.5331742167472839\n",
      "Batch 13 get train loss: 0.5826533436775208 val loss: 0.5025665760040283\n",
      "Batch 14 get train loss: 0.5834884643554688 val loss: 0.5404715538024902\n",
      "Batch 15 get train loss: 0.5739389657974243 val loss: 0.5053206086158752\n",
      "Batch 16 get train loss: 0.5752383470535278 val loss: 0.5033541321754456\n",
      "Batch 17 get train loss: 0.5616852641105652 val loss: 0.5626211762428284\n",
      "Batch 18 get train loss: 0.5335732102394104 val loss: 0.512315034866333\n",
      "Batch 19 get train loss: 0.4982908368110657 val loss: 0.538092851638794\n",
      "Batch 20 get train loss: 0.4654209017753601 val loss: 0.49839669466018677\n",
      "Batch 21 get train loss: 0.46830713748931885 val loss: 0.5050785541534424\n",
      "Batch 22 get train loss: 0.4071695804595947 val loss: 0.8230243921279907\n",
      "Batch 23 get train loss: 0.44750234484672546 val loss: 0.509483277797699\n",
      "Batch 24 get train loss: 0.3726063668727875 val loss: 0.5321704149246216\n",
      "Batch 25 get train loss: 0.31374889612197876 val loss: 0.5003750920295715\n",
      "Batch 26 get train loss: 0.27661633491516113 val loss: 0.5163397789001465\n",
      "Batch 27 get train loss: 0.24243396520614624 val loss: 0.5050684213638306\n",
      "Batch 28 get train loss: 0.21566203236579895 val loss: 0.6116399765014648\n",
      "Batch 29 get train loss: 0.2142377495765686 val loss: 0.5194604396820068\n"
     ]
    }
   ],
   "source": [
    "train(peft_model, optimizer, criterion, train_dataloader, eval_dataloader, device,epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 get train loss: 0.6353276371955872 val loss: 0.5333637595176697\n",
      "Batch 1 get train loss: 0.6056520342826843 val loss: 0.5178259611129761\n",
      "Batch 2 get train loss: 0.6068936586380005 val loss: 0.5324974656105042\n",
      "Batch 3 get train loss: 0.6043715476989746 val loss: 0.5260958671569824\n",
      "Batch 4 get train loss: 0.6008870601654053 val loss: 0.5142931342124939\n",
      "Batch 5 get train loss: 0.6099104285240173 val loss: 0.5510181188583374\n",
      "Batch 6 get train loss: 0.5951511263847351 val loss: 0.5079817175865173\n",
      "Batch 7 get train loss: 0.6089515686035156 val loss: 0.5311503410339355\n",
      "Batch 8 get train loss: 0.6029284596443176 val loss: 0.5400116443634033\n",
      "Batch 9 get train loss: 0.6032438278198242 val loss: 0.5250625014305115\n",
      "Batch 10 get train loss: 0.6046921610832214 val loss: 0.5064818263053894\n",
      "Batch 11 get train loss: 0.5955328345298767 val loss: 0.5295922756195068\n",
      "Batch 12 get train loss: 0.5906210541725159 val loss: 0.5331742167472839\n",
      "Batch 13 get train loss: 0.5826533436775208 val loss: 0.5025665760040283\n",
      "Batch 14 get train loss: 0.5834884643554688 val loss: 0.5404715538024902\n",
      "Batch 15 get train loss: 0.5739389657974243 val loss: 0.5053206086158752\n",
      "Batch 16 get train loss: 0.5752383470535278 val loss: 0.5033541321754456\n",
      "Batch 17 get train loss: 0.5616852641105652 val loss: 0.5626211762428284\n",
      "Batch 18 get train loss: 0.5335732102394104 val loss: 0.512315034866333\n",
      "Batch 19 get train loss: 0.4982908368110657 val loss: 0.538092851638794\n",
      "Batch 20 get train loss: 0.4654209017753601 val loss: 0.49839669466018677\n",
      "Batch 21 get train loss: 0.46830713748931885 val loss: 0.5050785541534424\n",
      "Batch 22 get train loss: 0.4071695804595947 val loss: 0.8230243921279907\n",
      "Batch 23 get train loss: 0.44750234484672546 val loss: 0.509483277797699\n",
      "Batch 24 get train loss: 0.3726063668727875 val loss: 0.5321704149246216\n",
      "Batch 25 get train loss: 0.31374889612197876 val loss: 0.5003750920295715\n",
      "Batch 26 get train loss: 0.27661633491516113 val loss: 0.5163397789001465\n",
      "Batch 27 get train loss: 0.24243396520614624 val loss: 0.5050684213638306\n",
      "Batch 28 get train loss: 0.21566203236579895 val loss: 0.6116399765014648\n",
      "Batch 29 get train loss: 0.2142377495765686 val loss: 0.5194604396820068\n"
     ]
    }
   ],
   "source": [
    "## First train of peft model\n",
    "train(peft_model, optimizer, criterion, train_dataloader, eval_dataloader, device,epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lora adapter\n",
    "adapter_save_path = 'lora_adapter/adapter_1'\n",
    "peft_model.save_pretrained(adapter_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_model_from_adapter type <class 'peft.peft_model.PeftModel'>\n",
      "original_model_copy type <class 'MLP_function.CNN3D'>\n",
      "if peft_model and peft_model_from_adapter have same results True\n"
     ]
    }
   ],
   "source": [
    "# load adapter\n",
    "model_name = 'lora_adapter/adapter_1'\n",
    "peft_model_from_adapter = peft.PeftModel.from_pretrained(original_model_copy, model_name)\n",
    "print('peft_model_from_adapter type',type(peft_model_from_adapter))\n",
    "print('original_model_copy type',type(original_model_copy))\n",
    "with torch.no_grad():\n",
    "    y_peft = peft_model(X.to(device))\n",
    "    y_loaded_adapter = peft_model_from_adapter(X.to(device))\n",
    "print('if peft_model and peft_model_from_adapter have same results:',torch.allclose(y_peft, y_loaded_adapter))\n",
    "del y_peft, y_loaded_adapter\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'lora_adapter/adapter_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/lora_adapter/adapter_1/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\peft\\config.py:257\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[1;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    258\u001b[0m         model_id,\n\u001b[0;32m    259\u001b[0m         CONFIG_NAME,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    279\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    280\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    281\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-682af984-1ff5f0b215b23c5c033a6a4b;a524a4f9-b592-41b6-baf2-7552b38cce58)\n\nRepository Not Found for url: https://huggingface.co/lora_adapter/adapter_1/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### Merging\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora_adapter/adapter_1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m unmerged_peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mpeft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_model_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore merging==========\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munmerged_peft_model type\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mtype\u001b[39m(unmerged_peft_model))\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\peft\\peft_model.py:479\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[1;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[1;32m--> 479\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[0;32m    489\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\FL\\lib\\site-packages\\peft\\config.py:263\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[1;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    258\u001b[0m             model_id,\n\u001b[0;32m    259\u001b[0m             CONFIG_NAME,\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[0;32m    261\u001b[0m         )\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'lora_adapter/adapter_1'"
     ]
    }
   ],
   "source": [
    "### Merging\n",
    "model_name = 'lora_adapter/adapter_1'\n",
    "unmerged_peft_model = peft.PeftModel.from_pretrained(original_model_copy, model_name)\n",
    "print('before merging==========')\n",
    "print('unmerged_peft_model type',type(unmerged_peft_model))\n",
    "print('original_model_copy type',type(original_model_copy))\n",
    "print('trainalbe parameter in unmerged_peft_model:')\n",
    "for name, param in unmerged_peft_model.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")\n",
    "\n",
    "print('after merging==========')\n",
    "\n",
    "merged_peft_model = unmerged_peft_model.merge_and_unload()  \n",
    "print('merged_peft_model type',type(merged_peft_model))\n",
    "print('unmerged_peft_model type',type(unmerged_peft_model))\n",
    "\n",
    "print('trainalbe parameter in merged_peft_model:')\n",
    "for name, param in merged_peft_model.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loRA model merged sucessfull: True\n"
     ]
    }
   ],
   "source": [
    "# Compare results of merged model and unmerged model\n",
    "y_unmerged = unmerged_peft_model(X.to(device))\n",
    "y_merged = merged_peft_model(X.to(device))\n",
    "print('loRA model merged sucessfull:',torch.allclose(y_unmerged, y_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check if merged model same as original model: True\n"
     ]
    }
   ],
   "source": [
    "y_module = module_copy(X.to(device))\n",
    "print('check if merged model same as original model:',torch.allclose(y_module, y_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module copy: <class 'MLP_function.CNN3D'>\n",
      "Layer: conv.0.weight | Trainable: False\n",
      "Layer: conv.0.bias | Trainable: False\n",
      "Layer: conv.3.weight | Trainable: False\n",
      "Layer: conv.3.bias | Trainable: False\n",
      "Layer: fc.0.weight | Trainable: False\n",
      "Layer: fc.0.bias | Trainable: False\n",
      "Layer: fc.2.weight | Trainable: False\n",
      "Layer: fc.2.bias | Trainable: False\n",
      "\n",
      "module: <class 'MLP_function.CNN3D'>\n",
      "Layer: conv.0.weight | Trainable: True\n",
      "Layer: conv.0.bias | Trainable: True\n",
      "Layer: conv.3.weight | Trainable: True\n",
      "Layer: conv.3.bias | Trainable: True\n",
      "Layer: fc.0.weight | Trainable: True\n",
      "Layer: fc.0.bias | Trainable: True\n",
      "Layer: fc.2.weight | Trainable: True\n",
      "Layer: fc.2.bias | Trainable: True\n"
     ]
    }
   ],
   "source": [
    "print('module copy:',type(module_copy))\n",
    "\n",
    "for name, param in module_copy.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")\n",
    "\n",
    "print('\\nmodule:',type(module))\n",
    "\n",
    "for name, param in module.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 集合完全相同： True\n",
      "所有参数值完全相同！\n"
     ]
    }
   ],
   "source": [
    "# 1. 比较 state_dict 的 key 集合\n",
    "orig_sd   = module_copy.state_dict()\n",
    "merged_sd = merged_peft_model.state_dict()\n",
    "\n",
    "orig_keys   = set(orig_sd.keys())\n",
    "merged_keys = set(merged_sd.keys())\n",
    "\n",
    "print(\"Key 集合完全相同：\", orig_keys == merged_keys)\n",
    "if orig_keys != merged_keys:\n",
    "    print(\"仅在原 model 中有的 keys:\", orig_keys - merged_keys)\n",
    "    print(\"仅在 merged_model 中有的 keys:\", merged_keys - orig_keys)\n",
    "\n",
    "#    通常 merge 后参数会变化；如果你只是想确认结构，shape 检查到这里就够了。\n",
    "for k in orig_keys & merged_keys:\n",
    "    a = orig_sd[k]\n",
    "    b = merged_sd[k]\n",
    "    if not torch.equal(a, b):\n",
    "        print(f\"参数值不同: {k} （或使用 torch.allclose 验证近似相等）\")\n",
    "        # 如果想看具体差异，可以打印范数／最大差值\n",
    "        diff = (a - b).abs()\n",
    "        print(f\"   max|Δ| = {diff.max():.3e}, mean|Δ| = {diff.mean():.3e}\")\n",
    "        # 只示例第一个不同的 key，然后跳出\n",
    "        break\n",
    "else:\n",
    "    print(\"所有参数值完全相同！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
